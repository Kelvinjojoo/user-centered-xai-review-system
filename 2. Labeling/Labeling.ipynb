{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b09ed28f88d86c0",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe916f29a5f4d1fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:56:54.806155Z",
     "start_time": "2025-10-18T08:56:49.471983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.9->textblob) (8.3.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.9->textblob) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from vaderSentiment) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->vaderSentiment) (2025.10.5)\n",
      "Requirement already satisfied: transformers in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kelvin jonathan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!pip install vaderSentiment\n",
    "!pip install transformers\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87c0ee706aaeaa93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:56:54.825385Z",
     "start_time": "2025-10-18T08:56:54.809581Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Text processing and ML\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "671fbaecea162d2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:56:55.221432Z",
     "start_time": "2025-10-18T08:56:54.834608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7673, 19)\n",
      "Columns: ['business_name', 'review_id', 'user_id', 'date', 'text', 'stars_review', 'stars_business_average', 'cleaned_text', 'original_length', 'cleaned_length', 'normalized_text', 'case_folded_text', 'tokens', 'tokens_ml', 'tokens_dl', 'tokens_bert', 'lemmatized_ml', 'lemmatized_dl', 'lemmatized_bert']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('yelp_review_preprocessed.csv')\n",
    "\n",
    "# Convert date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a36faef4f425e0",
   "metadata": {},
   "source": [
    "## Sentiment Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39e1ea70ce353f2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:57:01.416713Z",
     "start_time": "2025-10-18T08:56:55.236874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≠ Starting Enhanced Sentiment Analysis for Single Business...\n",
      "üîç 1. Calculating VADER sentiment...\n",
      "üîç 2. Calculating TextBlob sentiment...\n",
      "üîç 3. Calculating lexicon-based scores...\n",
      "üîç 4. Integrating star ratings...\n",
      "üîç 5. Creating combined sentiment score...\n",
      "üîç 6. Assigning final sentiment labels...\n",
      "üîç 7. Calculating sentiment confidence...\n",
      "\n",
      "üìä SENTIMENT ANALYSIS RESULTS:\n",
      "==================================================\n",
      "   POSITIVE: 6406 reviews (83.5%)\n",
      "   NEGATIVE: 722 reviews (9.4%)\n",
      "   NEUTRAL: 545 reviews (7.1%)\n",
      "\n",
      "‚≠ê SENTIMENT BY STAR RATING:\n",
      "sentiment_label  negative  neutral  positive\n",
      "stars_review                                \n",
      "1.0                   273        0         0\n",
      "2.0                   449        0         0\n",
      "3.0                     0      545       424\n",
      "4.0                     0        0      2337\n",
      "5.0                     0        0      3645\n"
     ]
    }
   ],
   "source": [
    "def enhanced_sentiment_analysis_single_business(df):\n",
    "    \"\"\"\n",
    "    Enhanced sentiment analysis optimized for single business dataset\n",
    "    Combines multiple approaches for robust labeling\n",
    "    \"\"\"\n",
    "    print(\"üé≠ Starting Enhanced Sentiment Analysis for Single Business...\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure we have text to analyze\n",
    "    df['analysis_text'] = df['lemmatized_dl'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "\n",
    "    # ============ 1. VADER Sentiment ============\n",
    "    print(\"üîç 1. Calculating VADER sentiment...\")\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def get_vader_scores(text):\n",
    "        scores = analyzer.polarity_scores(str(text))\n",
    "        return scores['compound'], scores['pos'], scores['neg'], scores['neu']\n",
    "\n",
    "    df[['vader_compound', 'vader_pos', 'vader_neg', 'vader_neu']] = df['analysis_text'].apply(\n",
    "        lambda x: pd.Series(get_vader_scores(x))\n",
    "    )\n",
    "\n",
    "    # ============ 2. TextBlob Sentiment ============\n",
    "    print(\"üîç 2. Calculating TextBlob sentiment...\")\n",
    "    def get_textblob_sentiment(text):\n",
    "        blob = TextBlob(str(text))\n",
    "        return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "    df[['textblob_polarity', 'textblob_subjectivity']] = df['analysis_text'].apply(\n",
    "        lambda x: pd.Series(get_textblob_sentiment(x))\n",
    "    )\n",
    "\n",
    "    # ============ 3. Lexicon-Based Scoring ============\n",
    "    print(\"üîç 3. Calculating lexicon-based scores...\")\n",
    "\n",
    "    # Enhanced sentiment lexicons\n",
    "    positive_words = {\n",
    "        'excellent', 'amazing', 'great', 'good', 'wonderful', 'fantastic', 'outstanding',\n",
    "        'perfect', 'friendly', 'clean', 'tasty', 'love', 'nice', 'delicious', 'awesome',\n",
    "        'fresh', 'perfectly', 'best', 'favorite', 'recommend', 'yummy', 'satisfied', 'crispy', 'juicy', 'tender', 'flavorful', 'savory', 'friendly'\n",
    "    }\n",
    "\n",
    "    negative_words = {\n",
    "        'terrible', 'awful', 'horrible', 'bad', 'poor', 'disappointing', 'worst',\n",
    "        'waste', 'rude', 'dirty', 'slow', 'cold', 'hate', 'overpriced', 'disgusting',\n",
    "        'mediocre', 'undercooked', 'salty', 'bland', 'stale', 'expensive', 'greasy', 'cold', 'undercooked', 'noisy', 'rude', 'burnt'\n",
    "    }\n",
    "\n",
    "    def lexicon_sentiment_score(text):\n",
    "        if not isinstance(text, str):\n",
    "            return 0\n",
    "        words = set(text.lower().split())\n",
    "        pos_count = len(words.intersection(positive_words))\n",
    "        neg_count = len(words.intersection(negative_words))\n",
    "        total = pos_count + neg_count\n",
    "\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        return (pos_count - neg_count) / total\n",
    "\n",
    "    df['lexicon_score'] = df['analysis_text'].apply(lexicon_sentiment_score)\n",
    "\n",
    "    # ============ 4. Star Rating Integration ============\n",
    "    print(\"üîç 4. Integrating star ratings...\")\n",
    "\n",
    "    # Convert stars to sentiment score (1-2: negative, 3: neutral, 4-5: positive)\n",
    "    def stars_to_sentiment_score(stars):\n",
    "        if stars >= 4:\n",
    "            return 1.0\n",
    "        elif stars <= 2:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    df['star_sentiment'] = df['stars_review'].apply(stars_to_sentiment_score)\n",
    "\n",
    "    # ============ 5. Combined Sentiment Score ============\n",
    "    print(\"üîç 5. Creating combined sentiment score...\")\n",
    "\n",
    "    # Weighted combination of all signals\n",
    "    df['combined_sentiment'] = (\n",
    "            0.3 * df['vader_compound'] +           # VADER is good for social media text\n",
    "            0.2 * df['textblob_polarity'] +        # TextBlob for general sentiment\n",
    "            0.2 * df['lexicon_score'] +            # Custom lexicon for restaurant context\n",
    "            0.3 * df['star_sentiment']             # Direct user rating (most important)\n",
    "    )\n",
    "\n",
    "    # ============ 6. Final Sentiment Labeling ============\n",
    "    print(\"üîç 6. Assigning final sentiment labels...\")\n",
    "\n",
    "    def assign_sentiment_label(row):\n",
    "        combined = row['combined_sentiment']\n",
    "        stars = row['stars_review']\n",
    "\n",
    "        # Rule-based approach with confidence\n",
    "        if combined >= 0.3 or stars >= 4:\n",
    "            return 'positive'\n",
    "        elif combined <= -0.3 or stars <= 2:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "    df['sentiment_label'] = df.apply(assign_sentiment_label, axis=1)\n",
    "\n",
    "    # ============ 7. Sentiment Confidence ============\n",
    "    print(\"üîç 7. Calculating sentiment confidence...\")\n",
    "\n",
    "    def calculate_confidence(row):\n",
    "        # Higher confidence when all signals agree\n",
    "        signals = [\n",
    "            np.sign(row['vader_compound']),\n",
    "            np.sign(row['textblob_polarity']),\n",
    "            np.sign(row['lexicon_score']),\n",
    "            np.sign(row['star_sentiment'])\n",
    "        ]\n",
    "\n",
    "        agreement = sum(1 for s in signals if s == np.sign(row['combined_sentiment']))\n",
    "        return agreement / len(signals)\n",
    "\n",
    "    df['sentiment_confidence'] = df.apply(calculate_confidence, axis=1)\n",
    "\n",
    "    # ============ 8. Analysis ============\n",
    "    print(\"\\nüìä SENTIMENT ANALYSIS RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    sentiment_counts = df['sentiment_label'].value_counts()\n",
    "    for label, count in sentiment_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   {label.upper()}: {count} reviews ({percentage:.1f}%)\")\n",
    "\n",
    "    # Sentiment distribution by stars\n",
    "    print(\"\\n‚≠ê SENTIMENT BY STAR RATING:\")\n",
    "    sentiment_by_stars = pd.crosstab(df['stars_review'], df['sentiment_label'])\n",
    "    print(sentiment_by_stars)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Perform enhanced sentiment analysis\n",
    "df = enhanced_sentiment_analysis_single_business(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e47713fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:57:01.471339Z",
     "start_time": "2025-10-18T08:57:01.444491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7673 entries, 0 to 7672\n",
      "Data columns (total 31 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   business_name           7673 non-null   object        \n",
      " 1   review_id               7673 non-null   object        \n",
      " 2   user_id                 7673 non-null   object        \n",
      " 3   date                    7673 non-null   datetime64[ns]\n",
      " 4   text                    7673 non-null   object        \n",
      " 5   stars_review            7673 non-null   float64       \n",
      " 6   stars_business_average  7673 non-null   float64       \n",
      " 7   cleaned_text            7672 non-null   object        \n",
      " 8   original_length         7673 non-null   int64         \n",
      " 9   cleaned_length          7673 non-null   int64         \n",
      " 10  normalized_text         7672 non-null   object        \n",
      " 11  case_folded_text        7672 non-null   object        \n",
      " 12  tokens                  7673 non-null   object        \n",
      " 13  tokens_ml               7673 non-null   object        \n",
      " 14  tokens_dl               7673 non-null   object        \n",
      " 15  tokens_bert             7673 non-null   object        \n",
      " 16  lemmatized_ml           7673 non-null   object        \n",
      " 17  lemmatized_dl           7673 non-null   object        \n",
      " 18  lemmatized_bert         7673 non-null   object        \n",
      " 19  analysis_text           7673 non-null   object        \n",
      " 20  vader_compound          7673 non-null   float64       \n",
      " 21  vader_pos               7673 non-null   float64       \n",
      " 22  vader_neg               7673 non-null   float64       \n",
      " 23  vader_neu               7673 non-null   float64       \n",
      " 24  textblob_polarity       7673 non-null   float64       \n",
      " 25  textblob_subjectivity   7673 non-null   float64       \n",
      " 26  lexicon_score           7673 non-null   int64         \n",
      " 27  star_sentiment          7673 non-null   float64       \n",
      " 28  combined_sentiment      7673 non-null   float64       \n",
      " 29  sentiment_label         7673 non-null   object        \n",
      " 30  sentiment_confidence    7673 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(11), int64(3), object(16)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a879b2241214c8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:57:01.511012Z",
     "start_time": "2025-10-18T08:57:01.480418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>stars_review</th>\n",
       "      <th>stars_business_average</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>original_length</th>\n",
       "      <th>cleaned_length</th>\n",
       "      <th>...</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <th>lexicon_score</th>\n",
       "      <th>star_sentiment</th>\n",
       "      <th>combined_sentiment</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>vHLTOsdILT7xgTu7TAWTUQ</td>\n",
       "      <td>417HF4q8ynnWtuJrkNax_g</td>\n",
       "      <td>2016-07-25 04:34:34</td>\n",
       "      <td>This place has amazing oysters and the BEST bl...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>This place has amazing oysters and the BEST bl...</td>\n",
       "      <td>556</td>\n",
       "      <td>546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.343162</td>\n",
       "      <td>0.531197</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666652</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>I90lP6oPICTkrhCFGQt5tA</td>\n",
       "      <td>1UAb3zZQeGX6fzZax5DY1A</td>\n",
       "      <td>2016-12-19 20:27:16</td>\n",
       "      <td>OH MY!! A must try. We had no idea there would...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OH MY A must try We had no idea there would be...</td>\n",
       "      <td>425</td>\n",
       "      <td>407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.633737</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>469eAl2fB069YTF_B5zW7w</td>\n",
       "      <td>p2kXD3gNu3N776C0WrmBjA</td>\n",
       "      <td>2018-08-23 20:58:39</td>\n",
       "      <td>The fried seafood was extremely hot. Very nice...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The fried seafood was extremely hot Very nice ...</td>\n",
       "      <td>530</td>\n",
       "      <td>498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.360098</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663950</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>aPpHBDs7Jiiq0sb9YutOhQ</td>\n",
       "      <td>7cDhfvTSH1wTxEmXMj_ChQ</td>\n",
       "      <td>2013-06-24 18:07:12</td>\n",
       "      <td>I love this place. I wish my stay was longer s...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I love this place I wish my stay was longer so...</td>\n",
       "      <td>193</td>\n",
       "      <td>188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.317273</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.642065</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>k9OG5kA5ebruSx_f1T-P-A</td>\n",
       "      <td>7QTh-fkw9Nr2lO10-PV8yw</td>\n",
       "      <td>2010-10-06 08:03:20</td>\n",
       "      <td>Loved the chargrilled oysters!  I mean, seriou...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Loved the chargrilled oysters I mean seriously...</td>\n",
       "      <td>1286</td>\n",
       "      <td>1229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.128603</td>\n",
       "      <td>0.601739</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320831</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       business_name               review_id                 user_id  \\\n",
       "0  Acme Oyster House  vHLTOsdILT7xgTu7TAWTUQ  417HF4q8ynnWtuJrkNax_g   \n",
       "1  Acme Oyster House  I90lP6oPICTkrhCFGQt5tA  1UAb3zZQeGX6fzZax5DY1A   \n",
       "2  Acme Oyster House  469eAl2fB069YTF_B5zW7w  p2kXD3gNu3N776C0WrmBjA   \n",
       "3  Acme Oyster House  aPpHBDs7Jiiq0sb9YutOhQ  7cDhfvTSH1wTxEmXMj_ChQ   \n",
       "4  Acme Oyster House  k9OG5kA5ebruSx_f1T-P-A  7QTh-fkw9Nr2lO10-PV8yw   \n",
       "\n",
       "                 date                                               text  \\\n",
       "0 2016-07-25 04:34:34  This place has amazing oysters and the BEST bl...   \n",
       "1 2016-12-19 20:27:16  OH MY!! A must try. We had no idea there would...   \n",
       "2 2018-08-23 20:58:39  The fried seafood was extremely hot. Very nice...   \n",
       "3 2013-06-24 18:07:12  I love this place. I wish my stay was longer s...   \n",
       "4 2010-10-06 08:03:20  Loved the chargrilled oysters!  I mean, seriou...   \n",
       "\n",
       "   stars_review  stars_business_average  \\\n",
       "0           5.0                     4.0   \n",
       "1           5.0                     4.0   \n",
       "2           5.0                     4.0   \n",
       "3           5.0                     4.0   \n",
       "4           3.0                     4.0   \n",
       "\n",
       "                                        cleaned_text  original_length  \\\n",
       "0  This place has amazing oysters and the BEST bl...              556   \n",
       "1  OH MY A must try We had no idea there would be...              425   \n",
       "2  The fried seafood was extremely hot Very nice ...              530   \n",
       "3  I love this place I wish my stay was longer so...              193   \n",
       "4  Loved the chargrilled oysters I mean seriously...             1286   \n",
       "\n",
       "   cleaned_length  ... vader_pos vader_neg vader_neu textblob_polarity  \\\n",
       "0             546  ...     0.514     0.022     0.464          0.343162   \n",
       "1             407  ...     0.410     0.091     0.499          0.214286   \n",
       "2             498  ...     0.421     0.000     0.579          0.360098   \n",
       "3             188  ...     0.461     0.000     0.539          0.317273   \n",
       "4            1229  ...     0.271     0.073     0.656          0.128603   \n",
       "\n",
       "  textblob_subjectivity lexicon_score star_sentiment combined_sentiment  \\\n",
       "0              0.531197             0            1.0           0.666652   \n",
       "1              0.614286             0            1.0           0.633737   \n",
       "2              0.519608             0            1.0           0.663950   \n",
       "3              0.490909             0            1.0           0.642065   \n",
       "4              0.601739             0            0.0           0.320831   \n",
       "\n",
       "  sentiment_label sentiment_confidence  \n",
       "0        positive                 0.75  \n",
       "1        positive                 0.75  \n",
       "2        positive                 0.75  \n",
       "3        positive                 0.75  \n",
       "4        positive                 0.50  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cce6fad5a539ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:58:46.993926Z",
     "start_time": "2025-10-18T08:58:46.962294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Baris yang memiliki nilai NULL:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>stars_review</th>\n",
       "      <th>stars_business_average</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>original_length</th>\n",
       "      <th>cleaned_length</th>\n",
       "      <th>...</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <th>lexicon_score</th>\n",
       "      <th>star_sentiment</th>\n",
       "      <th>combined_sentiment</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3787</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>3Jji_9v3aoKe2Dbi44lWXg</td>\n",
       "      <td>eArpCCLM_Bx33KpevzNyZw</td>\n",
       "      <td>2014-04-30 03:33:43</td>\n",
       "      <td>„Ç´„Ç≠„Çí„Åü„Åè„Åï„ÇìÈ£ü„Åπ„Åæ„Åó„Åü„ÄÇÂÆâ„Åè„Å¶ÁæéÂë≥„Åó„ÅÑ„Åß„Åô„ÄÇ„Ç´„Ç≠„ÅØÁîü„Ç¨„Ç≠„Åß„ÄÅ‰∏ÄÂÄãÁ¥Ñ1„Éâ„É´„ÄÇË™øÁêÜ„Åó„Åü„Ç´„Ç≠„ÅØ„ÄÅ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          business_name               review_id                 user_id  \\\n",
       "3787  Acme Oyster House  3Jji_9v3aoKe2Dbi44lWXg  eArpCCLM_Bx33KpevzNyZw   \n",
       "\n",
       "                    date                                               text  \\\n",
       "3787 2014-04-30 03:33:43  „Ç´„Ç≠„Çí„Åü„Åè„Åï„ÇìÈ£ü„Åπ„Åæ„Åó„Åü„ÄÇÂÆâ„Åè„Å¶ÁæéÂë≥„Åó„ÅÑ„Åß„Åô„ÄÇ„Ç´„Ç≠„ÅØÁîü„Ç¨„Ç≠„Åß„ÄÅ‰∏ÄÂÄãÁ¥Ñ1„Éâ„É´„ÄÇË™øÁêÜ„Åó„Åü„Ç´„Ç≠„ÅØ„ÄÅ...   \n",
       "\n",
       "      stars_review  stars_business_average cleaned_text  original_length  \\\n",
       "3787           4.0                     4.0          NaN              141   \n",
       "\n",
       "      cleaned_length  ... vader_pos vader_neg vader_neu textblob_polarity  \\\n",
       "3787               0  ...       0.0       0.0       1.0               0.0   \n",
       "\n",
       "     textblob_subjectivity lexicon_score star_sentiment combined_sentiment  \\\n",
       "3787                   0.0             0            1.0                0.3   \n",
       "\n",
       "     sentiment_label sentiment_confidence  \n",
       "3787        positive                 0.25  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_row = df[df[['cleaned_text', 'normalized_text', 'case_folded_text']].isna().any(axis=1)]\n",
    "print(\"üîç Baris yang memiliki nilai NULL:\")\n",
    "null_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b631b6ed5aaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bbfad3ba359ad0f",
   "metadata": {},
   "source": [
    "## Fake Review Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c67763712431d66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T08:45:19.279354Z",
     "start_time": "2025-10-18T08:45:17.204416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üïµÔ∏è Starting Enhanced Fake Review Detection with Dual Temporal Analysis...\n",
      "‚è∞ 1. Analyzing DUAL temporal patterns (daily + hourly)...\n",
      "üë§ 2. Analyzing user behavior patterns...\n",
      "üìù 3. Analyzing text quality and similarities...\n",
      "   üîç Calculating text similarities (this may take a while)...\n",
      "   ‚úÖ Text similarity analysis completed on 7673 samples\n",
      "‚≠ê 4. Detecting rating anomalies and patterns...\n",
      "üé≠ 5. Checking sentiment-rating consistency...\n",
      "üîó 6. Combining all detection signals with optimized weights...\n",
      "üè∑Ô∏è 7. Assigning final fake labels...\n",
      "üìã 8. Generating detailed explanations...\n",
      "\n",
      "üîé COMPREHENSIVE FAKE REVIEW DETECTION RESULTS:\n",
      "============================================================\n",
      "   üìä Total reviews analyzed: 7,673\n",
      "   üö® Potential fake reviews: 1151 (15.0%)\n",
      "   üìà Detection threshold: 0.409\n",
      "   üìÖ Analysis period: 2006-09-18 to 2022-01-17\n",
      "\n",
      "   üé≠ FAKE REVIEWS BY SENTIMENT:\n",
      "      POSITIVE: 846 reviews (73.5%)\n",
      "      NEGATIVE: 260 reviews (22.6%)\n",
      "      NEUTRAL: 45 reviews (3.9%)\n",
      "\n",
      "   ‚≠ê FAKE REVIEWS BY STAR RATING:\n",
      "      1.0 stars: 183 reviews (15.9%)\n",
      "      2.0 stars: 77 reviews (6.7%)\n",
      "      3.0 stars: 265 reviews (23.0%)\n",
      "      4.0 stars: 96 reviews (8.3%)\n",
      "      5.0 stars: 530 reviews (46.0%)\n",
      "\n",
      "   üîç TOP DETECTION REASONS:\n",
      "      - Medium daily spike (z=1.25); High hourly spike (z=3.59): 111 reviews\n",
      "      - High hourly spike (z=3.59): 109 reviews\n",
      "      - High daily spike (z=3.36): 106 reviews\n",
      "      - High daily spike (z=1.96); High hourly spike (z=3.59): 76 reviews\n",
      "      - High daily spike (z=2.66); High hourly spike (z=3.59): 68 reviews\n",
      "      - Medium sentiment-rating mismatch: 67 reviews\n",
      "      - High rating deviation: 59 reviews\n",
      "      - High daily spike (z=3.36); High hourly spike (z=3.59): 51 reviews\n",
      "      - Medium daily spike (z=1.25); High rating deviation: 38 reviews\n",
      "      - High daily spike (z=1.96); Medium sentiment-rating mismatch: 37 reviews\n",
      "      - Medium daily spike (z=1.25); Medium sentiment-rating mismatch: 36 reviews\n",
      "      - High daily spike (z=4.07): 36 reviews\n",
      "      - High daily spike (z=1.96); High hourly spike (z=7.43): 22 reviews\n",
      "      - High daily spike (z=4.77): 20 reviews\n",
      "      - High daily spike (z=4.07); High hourly spike (z=3.59): 20 reviews\n",
      "      - High daily spike (z=2.66); Medium sentiment-rating mismatch: 20 reviews\n",
      "      - High daily spike (z=1.96); High rating deviation: 16 reviews\n",
      "      - High daily spike (z=2.66): 16 reviews\n",
      "      - High daily spike (z=3.36); High hourly spike (z=7.43): 15 reviews\n",
      "      - High daily spike (z=4.07); High hourly spike (z=7.43): 14 reviews\n",
      "      - High hourly spike (z=3.59); High rating deviation: 13 reviews\n",
      "      - High hourly spike (z=3.59); Medium sentiment-rating mismatch: 13 reviews\n",
      "      - High daily spike (z=4.77); High hourly spike (z=3.59): 12 reviews\n",
      "      - Medium daily spike (z=1.25); High hourly spike (z=3.59); Medium sentiment-rating mismatch: 12 reviews\n",
      "      - Medium daily spike (z=1.25); High hourly spike (z=3.59); High rating deviation: 10 reviews\n",
      "      - High daily spike (z=2.66); High rating deviation: 9 reviews\n",
      "      - High daily spike (z=1.96); High hourly spike (z=3.59); High rating deviation: 9 reviews\n",
      "      - Extreme rating pattern: 9 reviews\n",
      "      - Medium daily spike (z=1.25); High hourly spike (z=7.43): 9 reviews\n",
      "      - High daily spike (z=2.66); High hourly spike (z=7.43): 8 reviews\n",
      "      - High daily spike (z=1.96); High hourly spike (z=3.59); Medium sentiment-rating mismatch: 8 reviews\n",
      "      - High daily spike (z=2.66); High hourly spike (z=3.59); High rating deviation: 6 reviews\n",
      "      - High hourly spike (z=7.43): 6 reviews\n",
      "      - High daily spike (z=3.36); Medium sentiment-rating mismatch: 6 reviews\n",
      "      - High daily spike (z=2.66); High hourly spike (z=3.59); Medium sentiment-rating mismatch: 5 reviews\n",
      "      - High daily spike (z=5.48): 5 reviews\n",
      "      - High daily spike (z=3.36); Extreme rating pattern: 5 reviews\n",
      "      - High daily spike (z=2.66); Extreme rating pattern: 4 reviews\n",
      "      - Medium daily spike (z=1.25); High hourly spike (z=3.59); Extreme rating pattern: 4 reviews\n",
      "      - High daily spike (z=4.07); Medium sentiment-rating mismatch: 4 reviews\n",
      "      - High daily spike (z=4.07); High rating deviation: 4 reviews\n",
      "      - High daily spike (z=3.36); High hourly spike (z=3.59); Medium sentiment-rating mismatch: 4 reviews\n",
      "      - High daily spike (z=1.96); Extreme rating pattern: 4 reviews\n",
      "      - High daily spike (z=3.36); High rating deviation: 4 reviews\n",
      "      - High daily spike (z=2.66); High hourly spike (z=7.43); High rating deviation: 3 reviews\n",
      "      - High rating deviation; Extreme rating pattern: 3 reviews\n",
      "      - High daily spike (z=5.48); High hourly spike (z=7.43): 3 reviews\n",
      "      - High daily spike (z=3.36); High hourly spike (z=3.59); Extreme rating pattern: 2 reviews\n",
      "      - Medium daily spike (z=1.25); High rating deviation; Extreme rating pattern: 2 reviews\n",
      "      - Medium daily spike (z=1.25); Extreme rating pattern: 2 reviews\n",
      "      - High daily spike (z=1.96); High hourly spike (z=3.59); Extreme rating pattern: 2 reviews\n",
      "      - High hourly spike (z=3.59); Extreme rating pattern: 2 reviews\n",
      "      - Extreme rating pattern; Medium sentiment-rating mismatch: 2 reviews\n",
      "      - High daily spike (z=5.48); High hourly spike (z=3.59): 2 reviews\n",
      "      - High daily spike (z=2.66); High hourly spike (z=7.43); Medium sentiment-rating mismatch: 1 reviews\n",
      "      - High daily spike (z=1.96): 1 reviews\n",
      "      - High daily spike (z=3.36); High hourly spike (z=3.59); High rating deviation: 1 reviews\n",
      "      - High daily spike (z=4.07); High hourly spike (z=3.59); Medium sentiment-rating mismatch: 1 reviews\n",
      "      - High daily spike (z=4.07); Extreme rating pattern: 1 reviews\n",
      "      - High daily spike (z=4.77); High rating deviation: 1 reviews\n",
      "      - High daily spike (z=1.96); High rating deviation; Extreme rating pattern: 1 reviews\n",
      "      - High daily spike (z=1.96); High hourly spike (z=7.43); High rating deviation: 1 reviews\n",
      "      - High daily spike (z=4.77); High hourly spike (z=3.59); High rating deviation: 1 reviews\n",
      "      - High daily spike (z=4.07); High hourly spike (z=3.59); High rating deviation: 1 reviews\n",
      "      - High daily spike (z=4.07); High hourly spike (z=7.43); Medium sentiment-rating mismatch: 1 reviews\n",
      "      - High daily spike (z=2.66); High hourly spike (z=3.59); Extreme rating pattern: 1 reviews\n",
      "      - High hourly spike (z=3.59); High rating deviation; Extreme rating pattern: 1 reviews\n",
      "      - High daily spike (z=4.77); Extreme rating pattern: 1 reviews\n",
      "      - High daily spike (z=4.77); High hourly spike (z=3.59); Extreme rating pattern: 1 reviews\n",
      "      - High daily spike (z=4.77); Medium sentiment-rating mismatch: 1 reviews\n",
      "      - Medium daily spike (z=1.25); High hourly spike (z=3.59); Extreme rating pattern; Medium sentiment-rating mismatch: 1 reviews\n",
      "      - High daily spike (z=1.96); High hourly spike (z=7.43); Medium sentiment-rating mismatch: 1 reviews\n"
     ]
    }
   ],
   "source": [
    "def enhanced_fake_review_detection_single_business(df):\n",
    "    \"\"\"\n",
    "    Enhanced fake review detection with DUAL temporal analysis (daily + hourly spikes)\n",
    "    Optimized for single business dataset\n",
    "    \"\"\"\n",
    "    print(\"\\nüïµÔ∏è Starting Enhanced Fake Review Detection with Dual Temporal Analysis...\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # ============ 1. DUAL TEMPORAL ANALYSIS ============\n",
    "    print(\"‚è∞ 1. Analyzing DUAL temporal patterns (daily + hourly)...\")\n",
    "\n",
    "    # Convert to datetime and sort\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    # Create temporal features\n",
    "    df['date_only'] = df['date'].dt.date\n",
    "    df['hour'] = df['date'].dt.floor('H')  # Round to hour\n",
    "    df['date_hour'] = df['date'].dt.strftime('%Y-%m-%d %H:00:00')  # Combine date and hour\n",
    "\n",
    "    # DAILY analysis\n",
    "    daily_counts = df.groupby('date_only').size()\n",
    "    df['daily_count'] = df['date_only'].map(daily_counts)\n",
    "\n",
    "    # HOURLY analysis - PERBAIKAN: perhatikan hari dan jam bersama-sama\n",
    "    hourly_counts = df.groupby('date_hour').size()\n",
    "    df['hourly_count'] = df['date_hour'].map(hourly_counts)\n",
    "\n",
    "    # Calculate z-scores for both temporal patterns\n",
    "    df['daily_spike_score'] = 0.0\n",
    "    df['hourly_spike_score'] = 0.0\n",
    "\n",
    "    # Daily z-score (if we have enough days)\n",
    "    if len(daily_counts) > 3:\n",
    "        try:\n",
    "            daily_z = zscore(daily_counts)\n",
    "            date_to_daily_z = dict(zip(daily_counts.index, daily_z))\n",
    "            df['daily_spike_score'] = df['date_only'].map(lambda x: max(date_to_daily_z.get(x, 0), 0))\n",
    "        except:\n",
    "            print(\"   ‚ö†Ô∏è Daily z-score calculation failed\")\n",
    "\n",
    "    # Hourly z-score - PERBAIKAN: berdasarkan date_hour (kombinasi tanggal dan jam)\n",
    "    if len(hourly_counts) > 5:\n",
    "        try:\n",
    "            hourly_z = zscore(hourly_counts)\n",
    "            date_hour_to_hourly_z = dict(zip(hourly_counts.index, hourly_z))\n",
    "            df['hourly_spike_score'] = df['date_hour'].map(lambda x: max(date_hour_to_hourly_z.get(x, 0), 0))\n",
    "        except:\n",
    "            print(\"   ‚ö†Ô∏è Hourly z-score calculation failed\")\n",
    "\n",
    "    # Combined temporal score (weighted)\n",
    "    df['temporal_spike_score'] = (\n",
    "            0.6 * df['daily_spike_score'] +  # Daily spikes are more significant\n",
    "            0.4 * df['hourly_spike_score']   # Hourly spikes are supporting evidence\n",
    "    )\n",
    "\n",
    "    # ============ 2. USER BEHAVIOR ANALYSIS ============\n",
    "    print(\"üë§ 2. Analyzing user behavior patterns...\")\n",
    "\n",
    "    user_stats = df.groupby('user_id').agg({\n",
    "        'review_id': 'count',\n",
    "        'date': ['min', 'max'],\n",
    "        'stars_review': 'mean',\n",
    "        'sentiment_label': lambda x: (x == 'positive').mean()\n",
    "    }).round(3)\n",
    "\n",
    "    # Flatten column names\n",
    "    user_stats.columns = ['user_review_count', 'first_review', 'last_review',\n",
    "                          'avg_user_rating', 'positive_ratio']\n",
    "\n",
    "    # User activity metrics\n",
    "    user_stats['activity_span_days'] = (user_stats['last_review'] - user_stats['first_review']).dt.days + 1\n",
    "    user_stats['review_frequency'] = user_stats['user_review_count'] / user_stats['activity_span_days']\n",
    "\n",
    "    # Detect suspicious user patterns\n",
    "    user_stats['suspicious_user'] = (\n",
    "            (user_stats['review_frequency'] > 3) |           # Too many reviews per day\n",
    "            (user_stats['user_review_count'] > 50)           # Too many total reviews\n",
    "    ).astype(int)\n",
    "\n",
    "    # Merge user stats back to main dataframe\n",
    "    df = df.merge(user_stats, on='user_id', how='left')\n",
    "\n",
    "    # User behavior score\n",
    "    df['user_behavior_score'] = (\n",
    "            0.4 * MinMaxScaler().fit_transform(df[['user_review_count']]).flatten() +\n",
    "            0.3 * MinMaxScaler().fit_transform(df[['review_frequency']]).flatten() +\n",
    "            0.2 * df['suspicious_user']\n",
    "    )\n",
    "\n",
    "    # ============ 3. TEXT QUALITY & SIMILARITY ANALYSIS ============\n",
    "    print(\"üìù 3. Analyzing text quality and similarities...\")\n",
    "\n",
    "    # Text length analysis\n",
    "    df['text_length'] = df['lemmatized_dl'].apply(\n",
    "        lambda x: len(x) if isinstance(x, list) else len(str(x).split())\n",
    "    )\n",
    "\n",
    "    # Text similarity analysis\n",
    "    print(\"   üîç Calculating text similarities (this may take a while)...\")\n",
    "\n",
    "    df['text_similarity_score'] = 0.0\n",
    "\n",
    "    try:\n",
    "        # Use a sample for efficiency while maintaining representativeness\n",
    "        sample_size = min(7673, len(df))\n",
    "        sample_indices = np.random.choice(df.index, sample_size, replace=False)\n",
    "        sample_texts = df.loc[sample_indices, 'analysis_text'].tolist()\n",
    "\n",
    "        # TF-IDF vectorization\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            stop_words='english',\n",
    "            min_df=2,\n",
    "            max_df=0.8\n",
    "        )\n",
    "\n",
    "        tfidf_matrix = vectorizer.fit_transform(sample_texts)\n",
    "        similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        similarity_scores = np.zeros(len(df))\n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            # Count reviews with high similarity (>0.85) excluding self\n",
    "            similar_indices = np.where(similarity_matrix[i] > 0.85)[0]\n",
    "            similar_count = len(similar_indices) - 1  # exclude self\n",
    "\n",
    "            if similar_count > 0:\n",
    "                # Normalize score based on number of similar reviews\n",
    "                score = min(similar_count / 5.0, 1.0)  # cap at 1.0\n",
    "                similarity_scores[idx] = score\n",
    "\n",
    "        df['text_similarity_score'] = similarity_scores\n",
    "        print(f\"   ‚úÖ Text similarity analysis completed on {sample_size} samples\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Text similarity calculation failed: {e}\")\n",
    "        df['text_similarity_score'] = 0.0\n",
    "\n",
    "    # ============ 4. RATING ANOMALIES & PATTERNS ============\n",
    "    print(\"‚≠ê 4. Detecting rating anomalies and patterns...\")\n",
    "\n",
    "    # Deviation from business average\n",
    "    business_avg = df['stars_business_average'].iloc[0]\n",
    "    df['rating_deviation'] = abs(df['stars_review'] - business_avg) / 4.0\n",
    "\n",
    "    # Extreme rater detection\n",
    "    user_rating_std = df.groupby('user_id')['stars_review'].std().fillna(0)\n",
    "    df['user_rating_consistency'] = df['user_id'].map(user_rating_std)\n",
    "    df['extreme_rater_score'] = ((df['user_rating_consistency'] < 0.5) &\n",
    "                                 (df['user_review_count'] > 1)).astype(int)\n",
    "\n",
    "    # Rating distribution analysis\n",
    "    rating_counts = df['stars_review'].value_counts().sort_index()\n",
    "    total_ratings = len(df)\n",
    "\n",
    "    # Detect rating manipulation (too many 5-star or 1-star reviews from same user)\n",
    "    user_rating_stats = df.groupby('user_id').agg({\n",
    "        'stars_review': ['count', lambda x: (x == 5).sum(), lambda x: (x == 1).sum()]\n",
    "    })\n",
    "    user_rating_stats.columns = ['total', 'five_star_count', 'one_star_count']\n",
    "    user_rating_stats['five_star_ratio'] = user_rating_stats['five_star_count'] / user_rating_stats['total']\n",
    "    user_rating_stats['one_star_ratio'] = user_rating_stats['one_star_count'] / user_rating_stats['total']\n",
    "\n",
    "    df['rating_manipulation_score'] = (\n",
    "                                              df['user_id'].map(user_rating_stats['five_star_ratio']).fillna(0) +\n",
    "                                              df['user_id'].map(user_rating_stats['one_star_ratio']).fillna(0)\n",
    "                                      ) / 2\n",
    "\n",
    "    # ============ 5. SENTIMENT-RATING CONSISTENCY ============\n",
    "    print(\"üé≠ 5. Checking sentiment-rating consistency...\")\n",
    "\n",
    "    def sentiment_rating_consistency(row):\n",
    "        sentiment = row['sentiment_label']\n",
    "        stars = row['stars_review']\n",
    "\n",
    "        # High inconsistency cases\n",
    "        if (sentiment == 'positive' and stars <= 2) or (sentiment == 'negative' and stars >= 4):\n",
    "            return 1.0\n",
    "        # Medium inconsistency\n",
    "        elif (sentiment == 'positive' and stars == 3) or (sentiment == 'negative' and stars == 3):\n",
    "            return 0.5\n",
    "        elif sentiment == 'neutral' and stars in [1, 5]:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    df['consistency_score'] = df.apply(sentiment_rating_consistency, axis=1)\n",
    "\n",
    "    # ============ 6. COMBINE ALL SIGNALS ============\n",
    "    print(\"üîó 6. Combining all detection signals with optimized weights...\")\n",
    "\n",
    "    # Optimized weights for single business analysis\n",
    "    weights = {\n",
    "        'temporal': 0.25,      # Daily + hourly spikes\n",
    "        'user_behavior': 0.25,  # User patterns\n",
    "        'text_similarity': 0.20, # Text copying\n",
    "        'rating_anomaly': 0.15,  # Rating deviations\n",
    "        'consistency': 0.15      # Sentiment-rating mismatch\n",
    "    }\n",
    "\n",
    "    # Calculate raw fake probability\n",
    "    df['fake_probability_raw'] = (\n",
    "            weights['temporal'] * MinMaxScaler().fit_transform(df[['temporal_spike_score']]).flatten() +\n",
    "            weights['user_behavior'] * df['user_behavior_score'] +\n",
    "            weights['text_similarity'] * df['text_similarity_score'] +\n",
    "            weights['rating_anomaly'] * (\n",
    "                    MinMaxScaler().fit_transform(df[['rating_deviation']]).flatten() * 0.6 +\n",
    "                    df['extreme_rater_score'] * 0.2 +\n",
    "                    MinMaxScaler().fit_transform(df[['rating_manipulation_score']]).flatten() * 0.12\n",
    "            ) +\n",
    "            weights['consistency'] * df['consistency_score']\n",
    "    )\n",
    "\n",
    "    # Scale to 0-1\n",
    "    df['fake_probability'] = MinMaxScaler().fit_transform(df[['fake_probability_raw']])\n",
    "\n",
    "    # ============ 7. FINAL FAKE LABEL ASSIGNMENT ============\n",
    "    print(\"üè∑Ô∏è 7. Assigning final fake labels...\")\n",
    "\n",
    "    # Use 85th percentile as threshold (more conservative)\n",
    "    threshold = df['fake_probability'].quantile(0.85)\n",
    "    df['is_fake'] = (df['fake_probability'] >= threshold).astype(int)\n",
    "\n",
    "    # ============ 8. ENHANCED EXPLAINABILITY ============\n",
    "    print(\"üìã 8. Generating detailed explanations...\")\n",
    "\n",
    "    def generate_detailed_explanation(row):\n",
    "        reasons = []\n",
    "\n",
    "        # Temporal reasons\n",
    "        if row['daily_spike_score'] > 1.5:\n",
    "            reasons.append(f\"High daily spike (z={row['daily_spike_score']:.2f})\")\n",
    "        elif row['daily_spike_score'] > 1.0:\n",
    "            reasons.append(f\"Medium daily spike (z={row['daily_spike_score']:.2f})\")\n",
    "\n",
    "        if row['hourly_spike_score'] > 2.0:\n",
    "            reasons.append(f\"High hourly spike (z={row['hourly_spike_score']:.2f})\")\n",
    "        elif row['hourly_spike_score'] > 1.5:\n",
    "            reasons.append(f\"Medium hourly spike (z={row['hourly_spike_score']:.2f})\")\n",
    "\n",
    "        # User behavior reasons\n",
    "        if row['user_behavior_score'] > 0.8:\n",
    "            reasons.append(\"Very suspicious user behavior\")\n",
    "        elif row['user_behavior_score'] > 0.6:\n",
    "            reasons.append(\"Suspicious user behavior\")\n",
    "\n",
    "        if row['suspicious_user'] == 1:\n",
    "            reasons.append(\"Flagged as suspicious user\")\n",
    "\n",
    "        # Text similarity reasons\n",
    "        if row['text_similarity_score'] > 0.7:\n",
    "            reasons.append(f\"High text similarity ({row['text_similarity_score']:.2f})\")\n",
    "        elif row['text_similarity_score'] > 0.4:\n",
    "            reasons.append(f\"Medium text similarity ({row['text_similarity_score']:.2f})\")\n",
    "\n",
    "        # Rating anomaly reasons\n",
    "        if row['rating_deviation'] > 0.75:\n",
    "            reasons.append(\"Extreme rating deviation\")\n",
    "        elif row['rating_deviation'] > 0.5:\n",
    "            reasons.append(\"High rating deviation\")\n",
    "\n",
    "        if row['extreme_rater_score'] == 1:\n",
    "            reasons.append(\"Extreme rating pattern\")\n",
    "\n",
    "        if row['rating_manipulation_score'] > 0.8:\n",
    "            reasons.append(\"Suspicious rating manipulation\")\n",
    "\n",
    "        # Consistency reasons\n",
    "        if row['consistency_score'] > 0.7:\n",
    "            reasons.append(\"High sentiment-rating mismatch\")\n",
    "        elif row['consistency_score'] > 0.4:\n",
    "            reasons.append(\"Medium sentiment-rating mismatch\")\n",
    "\n",
    "        return \"; \".join(reasons) if reasons else \"Normal review pattern\"\n",
    "\n",
    "    df['fake_explanation'] = df.apply(generate_detailed_explanation, axis=1)\n",
    "\n",
    "    # ============ 9. COMPREHENSIVE RESULTS ANALYSIS ============\n",
    "    print(\"\\nüîé COMPREHENSIVE FAKE REVIEW DETECTION RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    fake_count = df['is_fake'].sum()\n",
    "    fake_percentage = (fake_count / len(df)) * 100\n",
    "\n",
    "    print(f\"   üìä Total reviews analyzed: {len(df):,}\")\n",
    "    print(f\"   üö® Potential fake reviews: {fake_count} ({fake_percentage:.1f}%)\")\n",
    "    print(f\"   üìà Detection threshold: {threshold:.3f}\")\n",
    "    print(f\"   üìÖ Analysis period: {df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    # Fake reviews by sentiment\n",
    "    print(f\"\\n   üé≠ FAKE REVIEWS BY SENTIMENT:\")\n",
    "    fake_by_sentiment = df[df['is_fake'] == 1]['sentiment_label'].value_counts()\n",
    "    for sentiment, count in fake_by_sentiment.items():\n",
    "        percentage = (count / fake_count) * 100 if fake_count > 0 else 0\n",
    "        print(f\"      {sentiment.upper()}: {count} reviews ({percentage:.1f}%)\")\n",
    "\n",
    "    # Fake reviews by rating\n",
    "    print(f\"\\n   ‚≠ê FAKE REVIEWS BY STAR RATING:\")\n",
    "    fake_by_rating = df[df['is_fake'] == 1]['stars_review'].value_counts().sort_index()\n",
    "    for stars, count in fake_by_rating.items():\n",
    "        percentage = (count / fake_count) * 100 if fake_count > 0 else 0\n",
    "        print(f\"      {stars} stars: {count} reviews ({percentage:.1f}%)\")\n",
    "\n",
    "    # Top detection reasons\n",
    "    print(f\"\\n   üîç TOP DETECTION REASONS:\")\n",
    "    explanation_counts = df[df['is_fake'] == 1]['fake_explanation'].value_counts().head(100)\n",
    "    for reason, count in explanation_counts.items():\n",
    "        print(f\"      - {reason}: {count} reviews\")\n",
    "\n",
    "    return df, threshold\n",
    "\n",
    "# Perform enhanced fake review detection\n",
    "df, fake_threshold = enhanced_fake_review_detection_single_business(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99afe4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7673 entries, 0 to 7672\n",
      "Data columns (total 59 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   business_name              7673 non-null   object        \n",
      " 1   review_id                  7673 non-null   object        \n",
      " 2   user_id                    7673 non-null   object        \n",
      " 3   date                       7673 non-null   datetime64[ns]\n",
      " 4   text                       7673 non-null   object        \n",
      " 5   stars_review               7673 non-null   float64       \n",
      " 6   stars_business_average     7673 non-null   float64       \n",
      " 7   cleaned_text               7672 non-null   object        \n",
      " 8   original_length            7673 non-null   int64         \n",
      " 9   cleaned_length             7673 non-null   int64         \n",
      " 10  normalized_text            7672 non-null   object        \n",
      " 11  case_folded_text           7672 non-null   object        \n",
      " 12  tokens                     7673 non-null   object        \n",
      " 13  tokens_ml                  7673 non-null   object        \n",
      " 14  tokens_dl                  7673 non-null   object        \n",
      " 15  tokens_bert                7673 non-null   object        \n",
      " 16  lemmatized_ml              7673 non-null   object        \n",
      " 17  lemmatized_dl              7673 non-null   object        \n",
      " 18  lemmatized_bert            7673 non-null   object        \n",
      " 19  analysis_text              7673 non-null   object        \n",
      " 20  vader_compound             7673 non-null   float64       \n",
      " 21  vader_pos                  7673 non-null   float64       \n",
      " 22  vader_neg                  7673 non-null   float64       \n",
      " 23  vader_neu                  7673 non-null   float64       \n",
      " 24  textblob_polarity          7673 non-null   float64       \n",
      " 25  textblob_subjectivity      7673 non-null   float64       \n",
      " 26  lexicon_score              7673 non-null   int64         \n",
      " 27  star_sentiment             7673 non-null   float64       \n",
      " 28  combined_sentiment         7673 non-null   float64       \n",
      " 29  sentiment_label            7673 non-null   object        \n",
      " 30  sentiment_confidence       7673 non-null   float64       \n",
      " 31  date_only                  7673 non-null   object        \n",
      " 32  hour                       7673 non-null   datetime64[ns]\n",
      " 33  date_hour                  7673 non-null   object        \n",
      " 34  daily_count                7673 non-null   int64         \n",
      " 35  hourly_count               7673 non-null   int64         \n",
      " 36  daily_spike_score          7673 non-null   float64       \n",
      " 37  hourly_spike_score         7673 non-null   float64       \n",
      " 38  temporal_spike_score       7673 non-null   float64       \n",
      " 39  user_review_count          7673 non-null   int64         \n",
      " 40  first_review               7673 non-null   datetime64[ns]\n",
      " 41  last_review                7673 non-null   datetime64[ns]\n",
      " 42  avg_user_rating            7673 non-null   float64       \n",
      " 43  positive_ratio             7673 non-null   float64       \n",
      " 44  activity_span_days         7673 non-null   int64         \n",
      " 45  review_frequency           7673 non-null   float64       \n",
      " 46  suspicious_user            7673 non-null   int64         \n",
      " 47  user_behavior_score        7673 non-null   float64       \n",
      " 48  text_length                7673 non-null   int64         \n",
      " 49  text_similarity_score      7673 non-null   float64       \n",
      " 50  rating_deviation           7673 non-null   float64       \n",
      " 51  user_rating_consistency    7673 non-null   float64       \n",
      " 52  extreme_rater_score        7673 non-null   int64         \n",
      " 53  rating_manipulation_score  7673 non-null   float64       \n",
      " 54  consistency_score          7673 non-null   float64       \n",
      " 55  fake_probability_raw       7673 non-null   float64       \n",
      " 56  fake_probability           7673 non-null   float64       \n",
      " 57  is_fake                    7673 non-null   int64         \n",
      " 58  fake_explanation           7673 non-null   object        \n",
      "dtypes: datetime64[ns](4), float64(25), int64(11), object(19)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1873fea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>stars_review</th>\n",
       "      <th>stars_business_average</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>original_length</th>\n",
       "      <th>cleaned_length</th>\n",
       "      <th>...</th>\n",
       "      <th>text_similarity_score</th>\n",
       "      <th>rating_deviation</th>\n",
       "      <th>user_rating_consistency</th>\n",
       "      <th>extreme_rater_score</th>\n",
       "      <th>rating_manipulation_score</th>\n",
       "      <th>consistency_score</th>\n",
       "      <th>fake_probability_raw</th>\n",
       "      <th>fake_probability</th>\n",
       "      <th>is_fake</th>\n",
       "      <th>fake_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>DjEUxYAIbPmu9EnQMuso3A</td>\n",
       "      <td>Dd6ElS2Cng3Qag_h4IQC-Q</td>\n",
       "      <td>2006-09-18 22:01:13</td>\n",
       "      <td>Their Fried Peace Maker Po-Boy was delicious, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Their Fried Peace Maker Po Boy was delicious E...</td>\n",
       "      <td>179</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.375372</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium sentiment-rating mismatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>vYwBMm1uK9VgA735nFMCLQ</td>\n",
       "      <td>yeGIAyHixJrIe-zmXiePWQ</td>\n",
       "      <td>2006-10-01 00:35:10</td>\n",
       "      <td>Full disclosure here. I designed the Acme Oyst...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Full disclosure here I designed the Acme Oyste...</td>\n",
       "      <td>931</td>\n",
       "      <td>910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.072071</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal review pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>0FgDHLa41Qy4U9kZi_qlYQ</td>\n",
       "      <td>LyXvIE71cMeiBaPZI_Yq2w</td>\n",
       "      <td>2006-10-05 18:25:34</td>\n",
       "      <td>Try to go when they aren't too crowded (this m...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Try to go when they are not too crowded this m...</td>\n",
       "      <td>538</td>\n",
       "      <td>519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.072071</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal review pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>3L4xptZk0kgN3W8JgnqQsg</td>\n",
       "      <td>O1U20igtZ9ROL9WxHq3eng</td>\n",
       "      <td>2006-11-22 01:17:44</td>\n",
       "      <td>i don't normally like raw oysters cause they t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>i do not normally like raw oysters because the...</td>\n",
       "      <td>390</td>\n",
       "      <td>383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.210723</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal review pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>DvvBJtYhNzDEpSHRdg-BUQ</td>\n",
       "      <td>LHTsHRVgnhkBwagj81kHkQ</td>\n",
       "      <td>2006-11-26 12:00:47</td>\n",
       "      <td>dirty rice, po boys and oysters- the repeat bu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>dirty rice po boys and oysters the repeat but ...</td>\n",
       "      <td>220</td>\n",
       "      <td>214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.210723</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal review pattern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       business_name               review_id                 user_id  \\\n",
       "0  Acme Oyster House  DjEUxYAIbPmu9EnQMuso3A  Dd6ElS2Cng3Qag_h4IQC-Q   \n",
       "1  Acme Oyster House  vYwBMm1uK9VgA735nFMCLQ  yeGIAyHixJrIe-zmXiePWQ   \n",
       "2  Acme Oyster House  0FgDHLa41Qy4U9kZi_qlYQ  LyXvIE71cMeiBaPZI_Yq2w   \n",
       "3  Acme Oyster House  3L4xptZk0kgN3W8JgnqQsg  O1U20igtZ9ROL9WxHq3eng   \n",
       "4  Acme Oyster House  DvvBJtYhNzDEpSHRdg-BUQ  LHTsHRVgnhkBwagj81kHkQ   \n",
       "\n",
       "                 date                                               text  \\\n",
       "0 2006-09-18 22:01:13  Their Fried Peace Maker Po-Boy was delicious, ...   \n",
       "1 2006-10-01 00:35:10  Full disclosure here. I designed the Acme Oyst...   \n",
       "2 2006-10-05 18:25:34  Try to go when they aren't too crowded (this m...   \n",
       "3 2006-11-22 01:17:44  i don't normally like raw oysters cause they t...   \n",
       "4 2006-11-26 12:00:47  dirty rice, po boys and oysters- the repeat bu...   \n",
       "\n",
       "   stars_review  stars_business_average  \\\n",
       "0           3.0                     4.0   \n",
       "1           4.0                     4.0   \n",
       "2           4.0                     4.0   \n",
       "3           5.0                     4.0   \n",
       "4           5.0                     4.0   \n",
       "\n",
       "                                        cleaned_text  original_length  \\\n",
       "0  Their Fried Peace Maker Po Boy was delicious E...              179   \n",
       "1  Full disclosure here I designed the Acme Oyste...              931   \n",
       "2  Try to go when they are not too crowded this m...              538   \n",
       "3  i do not normally like raw oysters because the...              390   \n",
       "4  dirty rice po boys and oysters the repeat but ...              220   \n",
       "\n",
       "   cleaned_length  ... text_similarity_score rating_deviation  \\\n",
       "0             172  ...                   0.0             0.25   \n",
       "1             910  ...                   0.0             0.00   \n",
       "2             519  ...                   0.0             0.00   \n",
       "3             383  ...                   0.0             0.25   \n",
       "4             214  ...                   0.0             0.25   \n",
       "\n",
       "  user_rating_consistency extreme_rater_score rating_manipulation_score  \\\n",
       "0                     0.0                   0                       0.0   \n",
       "1                     0.0                   0                       0.0   \n",
       "2                     0.0                   0                       0.0   \n",
       "3                     0.0                   0                       0.5   \n",
       "4                     0.0                   0                       0.5   \n",
       "\n",
       "  consistency_score fake_probability_raw fake_probability is_fake  \\\n",
       "0               0.5                0.180         0.375372       0   \n",
       "1               0.0                0.075         0.072071       0   \n",
       "2               0.0                0.075         0.072071       0   \n",
       "3               0.0                0.123         0.210723       0   \n",
       "4               0.0                0.123         0.210723       0   \n",
       "\n",
       "                   fake_explanation  \n",
       "0  Medium sentiment-rating mismatch  \n",
       "1             Normal review pattern  \n",
       "2             Normal review pattern  \n",
       "3             Normal review pattern  \n",
       "4             Normal review pattern  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4173b715c4768af4",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e210da36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Baris yang memiliki nilai NULL:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>stars_review</th>\n",
       "      <th>stars_business_average</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>original_length</th>\n",
       "      <th>cleaned_length</th>\n",
       "      <th>...</th>\n",
       "      <th>text_similarity_score</th>\n",
       "      <th>rating_deviation</th>\n",
       "      <th>user_rating_consistency</th>\n",
       "      <th>extreme_rater_score</th>\n",
       "      <th>rating_manipulation_score</th>\n",
       "      <th>consistency_score</th>\n",
       "      <th>fake_probability_raw</th>\n",
       "      <th>fake_probability</th>\n",
       "      <th>is_fake</th>\n",
       "      <th>fake_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>3Jji_9v3aoKe2Dbi44lWXg</td>\n",
       "      <td>eArpCCLM_Bx33KpevzNyZw</td>\n",
       "      <td>2014-04-30 03:33:43</td>\n",
       "      <td>„Ç´„Ç≠„Çí„Åü„Åè„Åï„ÇìÈ£ü„Åπ„Åæ„Åó„Åü„ÄÇÂÆâ„Åè„Å¶ÁæéÂë≥„Åó„ÅÑ„Åß„Åô„ÄÇ„Ç´„Ç≠„ÅØÁîü„Ç¨„Ç≠„Åß„ÄÅ‰∏ÄÂÄãÁ¥Ñ1„Éâ„É´„ÄÇË™øÁêÜ„Åó„Åü„Ç´„Ç≠„ÅØ„ÄÅ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162385</td>\n",
       "      <td>0.32449</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium daily spike (z=1.25); High hourly spike...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          business_name               review_id                 user_id  \\\n",
       "1964  Acme Oyster House  3Jji_9v3aoKe2Dbi44lWXg  eArpCCLM_Bx33KpevzNyZw   \n",
       "\n",
       "                    date                                               text  \\\n",
       "1964 2014-04-30 03:33:43  „Ç´„Ç≠„Çí„Åü„Åè„Åï„ÇìÈ£ü„Åπ„Åæ„Åó„Åü„ÄÇÂÆâ„Åè„Å¶ÁæéÂë≥„Åó„ÅÑ„Åß„Åô„ÄÇ„Ç´„Ç≠„ÅØÁîü„Ç¨„Ç≠„Åß„ÄÅ‰∏ÄÂÄãÁ¥Ñ1„Éâ„É´„ÄÇË™øÁêÜ„Åó„Åü„Ç´„Ç≠„ÅØ„ÄÅ...   \n",
       "\n",
       "      stars_review  stars_business_average cleaned_text  original_length  \\\n",
       "1964           4.0                     4.0          NaN              141   \n",
       "\n",
       "      cleaned_length  ... text_similarity_score rating_deviation  \\\n",
       "1964               0  ...                   0.0              0.0   \n",
       "\n",
       "     user_rating_consistency extreme_rater_score rating_manipulation_score  \\\n",
       "1964                     0.0                   0                       0.0   \n",
       "\n",
       "     consistency_score fake_probability_raw fake_probability is_fake  \\\n",
       "1964               0.0             0.162385          0.32449       0   \n",
       "\n",
       "                                       fake_explanation  \n",
       "1964  Medium daily spike (z=1.25); High hourly spike...  \n",
       "\n",
       "[1 rows x 59 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Cek baris yang ada missing value di kolom terkait\n",
    "null_row = df[df[['cleaned_text', 'normalized_text', 'case_folded_text']].isna().any(axis=1)]\n",
    "print(\"üîç Baris yang memiliki nilai NULL:\")\n",
    "null_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a68ad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Baris NULL dihapus. Total data sekarang: 7672 baris.\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Hapus baris tersebut\n",
    "df = df.dropna(subset=['cleaned_text', 'normalized_text', 'case_folded_text']).reset_index(drop=True)\n",
    "print(f\"\\n‚úÖ Baris NULL dihapus. Total data sekarang: {len(df)} baris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7d78dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File berhasil disimpan ke: yelp_labeled_full.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"yelp_labeled_full.csv\"\n",
    "\n",
    "# Simpan ke CSV\n",
    "df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ File berhasil disimpan ke: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90044d21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:15:29.146618Z",
     "start_time": "2025-10-18T09:15:28.696238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INFO DATASET ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7672 entries, 0 to 7671\n",
      "Data columns (total 59 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   business_name              7672 non-null   object \n",
      " 1   review_id                  7672 non-null   object \n",
      " 2   user_id                    7672 non-null   object \n",
      " 3   date                       7672 non-null   object \n",
      " 4   text                       7672 non-null   object \n",
      " 5   stars_review               7672 non-null   float64\n",
      " 6   stars_business_average     7672 non-null   float64\n",
      " 7   cleaned_text               7672 non-null   object \n",
      " 8   original_length            7672 non-null   int64  \n",
      " 9   cleaned_length             7672 non-null   int64  \n",
      " 10  normalized_text            7672 non-null   object \n",
      " 11  case_folded_text           7672 non-null   object \n",
      " 12  tokens                     7672 non-null   object \n",
      " 13  tokens_ml                  7672 non-null   object \n",
      " 14  tokens_dl                  7672 non-null   object \n",
      " 15  tokens_bert                7672 non-null   object \n",
      " 16  lemmatized_ml              7672 non-null   object \n",
      " 17  lemmatized_dl              7672 non-null   object \n",
      " 18  lemmatized_bert            7672 non-null   object \n",
      " 19  analysis_text              7672 non-null   object \n",
      " 20  vader_compound             7672 non-null   float64\n",
      " 21  vader_pos                  7672 non-null   float64\n",
      " 22  vader_neg                  7672 non-null   float64\n",
      " 23  vader_neu                  7672 non-null   float64\n",
      " 24  textblob_polarity          7672 non-null   float64\n",
      " 25  textblob_subjectivity      7672 non-null   float64\n",
      " 26  lexicon_score              7672 non-null   int64  \n",
      " 27  star_sentiment             7672 non-null   float64\n",
      " 28  combined_sentiment         7672 non-null   float64\n",
      " 29  sentiment_label            7672 non-null   object \n",
      " 30  sentiment_confidence       7672 non-null   float64\n",
      " 31  date_only                  7672 non-null   object \n",
      " 32  hour                       7672 non-null   object \n",
      " 33  date_hour                  7672 non-null   object \n",
      " 34  daily_count                7672 non-null   int64  \n",
      " 35  hourly_count               7672 non-null   int64  \n",
      " 36  daily_spike_score          7672 non-null   float64\n",
      " 37  hourly_spike_score         7672 non-null   float64\n",
      " 38  temporal_spike_score       7672 non-null   float64\n",
      " 39  user_review_count          7672 non-null   int64  \n",
      " 40  first_review               7672 non-null   object \n",
      " 41  last_review                7672 non-null   object \n",
      " 42  avg_user_rating            7672 non-null   float64\n",
      " 43  positive_ratio             7672 non-null   float64\n",
      " 44  activity_span_days         7672 non-null   int64  \n",
      " 45  review_frequency           7672 non-null   float64\n",
      " 46  suspicious_user            7672 non-null   int64  \n",
      " 47  user_behavior_score        7672 non-null   float64\n",
      " 48  text_length                7672 non-null   int64  \n",
      " 49  text_similarity_score      7672 non-null   float64\n",
      " 50  rating_deviation           7672 non-null   float64\n",
      " 51  user_rating_consistency    7672 non-null   float64\n",
      " 52  extreme_rater_score        7672 non-null   int64  \n",
      " 53  rating_manipulation_score  7672 non-null   float64\n",
      " 54  consistency_score          7672 non-null   float64\n",
      " 55  fake_probability_raw       7672 non-null   float64\n",
      " 56  fake_probability           7672 non-null   float64\n",
      " 57  is_fake                    7672 non-null   int64  \n",
      " 58  fake_explanation           7672 non-null   object \n",
      "dtypes: float64(25), int64(11), object(23)\n",
      "memory usage: 3.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Baca ulang\n",
    "df_loaded = pd.read_csv(\"yelp_labeled_full.csv\")\n",
    "\n",
    "# Lihat info dasar\n",
    "print(\"=== INFO DATASET ===\")\n",
    "print(df_loaded.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c12c5b161510b207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:16:39.751928Z",
     "start_time": "2025-10-18T09:16:39.718420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_name</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>stars_review</th>\n",
       "      <th>stars_business_average</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>original_length</th>\n",
       "      <th>cleaned_length</th>\n",
       "      <th>...</th>\n",
       "      <th>text_similarity_score</th>\n",
       "      <th>rating_deviation</th>\n",
       "      <th>user_rating_consistency</th>\n",
       "      <th>extreme_rater_score</th>\n",
       "      <th>rating_manipulation_score</th>\n",
       "      <th>consistency_score</th>\n",
       "      <th>fake_probability_raw</th>\n",
       "      <th>fake_probability</th>\n",
       "      <th>is_fake</th>\n",
       "      <th>fake_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>DjEUxYAIbPmu9EnQMuso3A</td>\n",
       "      <td>Dd6ElS2Cng3Qag_h4IQC-Q</td>\n",
       "      <td>2006-09-18 22:01:13</td>\n",
       "      <td>Their Fried Peace Maker Po-Boy was delicious, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Their Fried Peace Maker Po Boy was delicious E...</td>\n",
       "      <td>179</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.375372</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium sentiment-rating mismatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>vYwBMm1uK9VgA735nFMCLQ</td>\n",
       "      <td>yeGIAyHixJrIe-zmXiePWQ</td>\n",
       "      <td>2006-10-01 00:35:10</td>\n",
       "      <td>Full disclosure here. I designed the Acme Oyst...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Full disclosure here I designed the Acme Oyste...</td>\n",
       "      <td>931</td>\n",
       "      <td>910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.072071</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal review pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>0FgDHLa41Qy4U9kZi_qlYQ</td>\n",
       "      <td>LyXvIE71cMeiBaPZI_Yq2w</td>\n",
       "      <td>2006-10-05 18:25:34</td>\n",
       "      <td>Try to go when they aren't too crowded (this m...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Try to go when they are not too crowded this m...</td>\n",
       "      <td>538</td>\n",
       "      <td>519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.072071</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal review pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>3L4xptZk0kgN3W8JgnqQsg</td>\n",
       "      <td>O1U20igtZ9ROL9WxHq3eng</td>\n",
       "      <td>2006-11-22 01:17:44</td>\n",
       "      <td>i don't normally like raw oysters cause they t...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>i do not normally like raw oysters because the...</td>\n",
       "      <td>390</td>\n",
       "      <td>383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.210723</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal review pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>DvvBJtYhNzDEpSHRdg-BUQ</td>\n",
       "      <td>LHTsHRVgnhkBwagj81kHkQ</td>\n",
       "      <td>2006-11-26 12:00:47</td>\n",
       "      <td>dirty rice, po boys and oysters- the repeat bu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>dirty rice po boys and oysters the repeat but ...</td>\n",
       "      <td>220</td>\n",
       "      <td>214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.210723</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal review pattern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       business_name               review_id                 user_id  \\\n",
       "0  Acme Oyster House  DjEUxYAIbPmu9EnQMuso3A  Dd6ElS2Cng3Qag_h4IQC-Q   \n",
       "1  Acme Oyster House  vYwBMm1uK9VgA735nFMCLQ  yeGIAyHixJrIe-zmXiePWQ   \n",
       "2  Acme Oyster House  0FgDHLa41Qy4U9kZi_qlYQ  LyXvIE71cMeiBaPZI_Yq2w   \n",
       "3  Acme Oyster House  3L4xptZk0kgN3W8JgnqQsg  O1U20igtZ9ROL9WxHq3eng   \n",
       "4  Acme Oyster House  DvvBJtYhNzDEpSHRdg-BUQ  LHTsHRVgnhkBwagj81kHkQ   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2006-09-18 22:01:13  Their Fried Peace Maker Po-Boy was delicious, ...   \n",
       "1  2006-10-01 00:35:10  Full disclosure here. I designed the Acme Oyst...   \n",
       "2  2006-10-05 18:25:34  Try to go when they aren't too crowded (this m...   \n",
       "3  2006-11-22 01:17:44  i don't normally like raw oysters cause they t...   \n",
       "4  2006-11-26 12:00:47  dirty rice, po boys and oysters- the repeat bu...   \n",
       "\n",
       "   stars_review  stars_business_average  \\\n",
       "0           3.0                     4.0   \n",
       "1           4.0                     4.0   \n",
       "2           4.0                     4.0   \n",
       "3           5.0                     4.0   \n",
       "4           5.0                     4.0   \n",
       "\n",
       "                                        cleaned_text  original_length  \\\n",
       "0  Their Fried Peace Maker Po Boy was delicious E...              179   \n",
       "1  Full disclosure here I designed the Acme Oyste...              931   \n",
       "2  Try to go when they are not too crowded this m...              538   \n",
       "3  i do not normally like raw oysters because the...              390   \n",
       "4  dirty rice po boys and oysters the repeat but ...              220   \n",
       "\n",
       "   cleaned_length  ... text_similarity_score rating_deviation  \\\n",
       "0             172  ...                   0.0             0.25   \n",
       "1             910  ...                   0.0             0.00   \n",
       "2             519  ...                   0.0             0.00   \n",
       "3             383  ...                   0.0             0.25   \n",
       "4             214  ...                   0.0             0.25   \n",
       "\n",
       "  user_rating_consistency extreme_rater_score rating_manipulation_score  \\\n",
       "0                     0.0                   0                       0.0   \n",
       "1                     0.0                   0                       0.0   \n",
       "2                     0.0                   0                       0.0   \n",
       "3                     0.0                   0                       0.5   \n",
       "4                     0.0                   0                       0.5   \n",
       "\n",
       "  consistency_score fake_probability_raw fake_probability is_fake  \\\n",
       "0               0.5                0.180         0.375372       0   \n",
       "1               0.0                0.075         0.072071       0   \n",
       "2               0.0                0.075         0.072071       0   \n",
       "3               0.0                0.123         0.210723       0   \n",
       "4               0.0                0.123         0.210723       0   \n",
       "\n",
       "                   fake_explanation  \n",
       "0  Medium sentiment-rating mismatch  \n",
       "1             Normal review pattern  \n",
       "2             Normal review pattern  \n",
       "3             Normal review pattern  \n",
       "4             Normal review pattern  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loaded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ccfb9",
   "metadata": {},
   "source": [
    "### Check review yang sama persis --> taunya dari pas preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d479bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3846</th>\n",
       "      <th>3851</th>\n",
       "      <th>3937</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business_name</th>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>Acme Oyster House</td>\n",
       "      <td>Acme Oyster House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <td>R0LuveEyadjqDBYEL4gqsw</td>\n",
       "      <td>NGff0grXrZMTBNqDkdFzJg</td>\n",
       "      <td>nY1p0rdzItxhzKJmyPknrQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>CaFbx2zBXiAUC-JmgsyDnQ</td>\n",
       "      <td>CaFbx2zBXiAUC-JmgsyDnQ</td>\n",
       "      <td>CaFbx2zBXiAUC-JmgsyDnQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>2016-10-23 21:01:00</td>\n",
       "      <td>2016-10-26 02:07:52</td>\n",
       "      <td>2016-12-04 01:17:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>I've never been more disrespected in my life, ...</td>\n",
       "      <td>I've never been more disrespected in my life, ...</td>\n",
       "      <td>I've never been more disrespected in my life, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars_review</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars_business_average</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleaned_text</th>\n",
       "      <td>I have never been more disrespected in my life...</td>\n",
       "      <td>I have never been more disrespected in my life...</td>\n",
       "      <td>I have never been more disrespected in my life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original_length</th>\n",
       "      <td>609</td>\n",
       "      <td>609</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleaned_length</th>\n",
       "      <td>590</td>\n",
       "      <td>590</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>normalized_text</th>\n",
       "      <td>I have never been more disrespected in my life...</td>\n",
       "      <td>I have never been more disrespected in my life...</td>\n",
       "      <td>I have never been more disrespected in my life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_folded_text</th>\n",
       "      <td>i have never been more disrespected in my life...</td>\n",
       "      <td>i have never been more disrespected in my life...</td>\n",
       "      <td>i have never been more disrespected in my life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>['i', 'have', 'never', 'been', 'more', 'disres...</td>\n",
       "      <td>['i', 'have', 'never', 'been', 'more', 'disres...</td>\n",
       "      <td>['i', 'have', 'never', 'been', 'more', 'disres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_ml</th>\n",
       "      <td>['never', 'disrespected', 'life', 'first', 'co...</td>\n",
       "      <td>['never', 'disrespected', 'life', 'first', 'co...</td>\n",
       "      <td>['never', 'disrespected', 'life', 'first', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_dl</th>\n",
       "      <td>['never', 'disrespected', 'life', 'first', 'co...</td>\n",
       "      <td>['never', 'disrespected', 'life', 'first', 'co...</td>\n",
       "      <td>['never', 'disrespected', 'life', 'first', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_bert</th>\n",
       "      <td>['i', 'have', 'never', 'been', 'more', 'disres...</td>\n",
       "      <td>['i', 'have', 'never', 'been', 'more', 'disres...</td>\n",
       "      <td>['i', 'have', 'never', 'been', 'more', 'disres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatized_ml</th>\n",
       "      <td>['never', 'disrespect', 'life', 'first', 'cous...</td>\n",
       "      <td>['never', 'disrespect', 'life', 'first', 'cous...</td>\n",
       "      <td>['never', 'disrespect', 'life', 'first', 'cous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatized_dl</th>\n",
       "      <td>['never', 'disrespect', 'life', 'first', 'cous...</td>\n",
       "      <td>['never', 'disrespect', 'life', 'first', 'cous...</td>\n",
       "      <td>['never', 'disrespect', 'life', 'first', 'cous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lemmatized_bert</th>\n",
       "      <td>['i', 'have', 'never', 'be', 'more', 'disrespe...</td>\n",
       "      <td>['i', 'have', 'never', 'be', 'more', 'disrespe...</td>\n",
       "      <td>['i', 'have', 'never', 'be', 'more', 'disrespe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analysis_text</th>\n",
       "      <td>['never', 'disrespect', 'life', 'first', 'cous...</td>\n",
       "      <td>['never', 'disrespect', 'life', 'first', 'cous...</td>\n",
       "      <td>['never', 'disrespect', 'life', 'first', 'cous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_compound</th>\n",
       "      <td>0.5774</td>\n",
       "      <td>0.5774</td>\n",
       "      <td>0.5774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_pos</th>\n",
       "      <td>0.184</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_neg</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vader_neu</th>\n",
       "      <td>0.722</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_polarity</th>\n",
       "      <td>0.225</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexicon_score</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>star_sentiment</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>combined_sentiment</th>\n",
       "      <td>-0.08178</td>\n",
       "      <td>-0.08178</td>\n",
       "      <td>-0.08178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_label</th>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_only</th>\n",
       "      <td>2016-10-23</td>\n",
       "      <td>2016-10-26</td>\n",
       "      <td>2016-12-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>2016-10-23 21:00:00</td>\n",
       "      <td>2016-10-26 02:00:00</td>\n",
       "      <td>2016-12-04 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date_hour</th>\n",
       "      <td>2016-10-23 21:00:00</td>\n",
       "      <td>2016-10-26 02:00:00</td>\n",
       "      <td>2016-12-04 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daily_count</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hourly_count</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daily_spike_score</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.546965</td>\n",
       "      <td>1.955602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hourly_spike_score</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temporal_spike_score</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.328179</td>\n",
       "      <td>1.173361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_review_count</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_review</th>\n",
       "      <td>2016-10-23 21:01:00</td>\n",
       "      <td>2016-10-23 21:01:00</td>\n",
       "      <td>2016-10-23 21:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_review</th>\n",
       "      <td>2016-12-04 01:17:31</td>\n",
       "      <td>2016-12-04 01:17:31</td>\n",
       "      <td>2016-12-04 01:17:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_user_rating</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive_ratio</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_span_days</th>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_frequency</th>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suspicious_user</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_behavior_score</th>\n",
       "      <td>0.421237</td>\n",
       "      <td>0.421237</td>\n",
       "      <td>0.421237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_length</th>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_similarity_score</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating_deviation</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_rating_consistency</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extreme_rater_score</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating_manipulation_score</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consistency_score</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake_probability_raw</th>\n",
       "      <td>0.323309</td>\n",
       "      <td>0.336418</td>\n",
       "      <td>0.370178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake_probability</th>\n",
       "      <td>0.789334</td>\n",
       "      <td>0.8272</td>\n",
       "      <td>0.924719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_fake</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake_explanation</th>\n",
       "      <td>High rating deviation; Extreme rating pattern</td>\n",
       "      <td>High rating deviation; Extreme rating pattern</td>\n",
       "      <td>High daily spike (z=1.96); High rating deviati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        3846  \\\n",
       "business_name                                              Acme Oyster House   \n",
       "review_id                                             R0LuveEyadjqDBYEL4gqsw   \n",
       "user_id                                               CaFbx2zBXiAUC-JmgsyDnQ   \n",
       "date                                                     2016-10-23 21:01:00   \n",
       "text                       I've never been more disrespected in my life, ...   \n",
       "stars_review                                                             1.0   \n",
       "stars_business_average                                                   4.0   \n",
       "cleaned_text               I have never been more disrespected in my life...   \n",
       "original_length                                                          609   \n",
       "cleaned_length                                                           590   \n",
       "normalized_text            I have never been more disrespected in my life...   \n",
       "case_folded_text           i have never been more disrespected in my life...   \n",
       "tokens                     ['i', 'have', 'never', 'been', 'more', 'disres...   \n",
       "tokens_ml                  ['never', 'disrespected', 'life', 'first', 'co...   \n",
       "tokens_dl                  ['never', 'disrespected', 'life', 'first', 'co...   \n",
       "tokens_bert                ['i', 'have', 'never', 'been', 'more', 'disres...   \n",
       "lemmatized_ml              ['never', 'disrespect', 'life', 'first', 'cous...   \n",
       "lemmatized_dl              ['never', 'disrespect', 'life', 'first', 'cous...   \n",
       "lemmatized_bert            ['i', 'have', 'never', 'be', 'more', 'disrespe...   \n",
       "analysis_text              ['never', 'disrespect', 'life', 'first', 'cous...   \n",
       "vader_compound                                                        0.5774   \n",
       "vader_pos                                                              0.184   \n",
       "vader_neg                                                              0.094   \n",
       "vader_neu                                                              0.722   \n",
       "textblob_polarity                                                      0.225   \n",
       "textblob_subjectivity                                               0.316667   \n",
       "lexicon_score                                                              0   \n",
       "star_sentiment                                                          -1.0   \n",
       "combined_sentiment                                                  -0.08178   \n",
       "sentiment_label                                                     negative   \n",
       "sentiment_confidence                                                    0.25   \n",
       "date_only                                                         2016-10-23   \n",
       "hour                                                     2016-10-23 21:00:00   \n",
       "date_hour                                                2016-10-23 21:00:00   \n",
       "daily_count                                                                2   \n",
       "hourly_count                                                               1   \n",
       "daily_spike_score                                                        0.0   \n",
       "hourly_spike_score                                                       0.0   \n",
       "temporal_spike_score                                                     0.0   \n",
       "user_review_count                                                          3   \n",
       "first_review                                             2016-10-23 21:01:00   \n",
       "last_review                                              2016-12-04 01:17:31   \n",
       "avg_user_rating                                                          1.0   \n",
       "positive_ratio                                                           0.0   \n",
       "activity_span_days                                                        42   \n",
       "review_frequency                                                    0.071429   \n",
       "suspicious_user                                                            0   \n",
       "user_behavior_score                                                 0.421237   \n",
       "text_length                                                               53   \n",
       "text_similarity_score                                                    0.4   \n",
       "rating_deviation                                                        0.75   \n",
       "user_rating_consistency                                                  0.0   \n",
       "extreme_rater_score                                                        1   \n",
       "rating_manipulation_score                                                0.5   \n",
       "consistency_score                                                        0.0   \n",
       "fake_probability_raw                                                0.323309   \n",
       "fake_probability                                                    0.789334   \n",
       "is_fake                                                                    1   \n",
       "fake_explanation               High rating deviation; Extreme rating pattern   \n",
       "\n",
       "                                                                        3851  \\\n",
       "business_name                                              Acme Oyster House   \n",
       "review_id                                             NGff0grXrZMTBNqDkdFzJg   \n",
       "user_id                                               CaFbx2zBXiAUC-JmgsyDnQ   \n",
       "date                                                     2016-10-26 02:07:52   \n",
       "text                       I've never been more disrespected in my life, ...   \n",
       "stars_review                                                             1.0   \n",
       "stars_business_average                                                   4.0   \n",
       "cleaned_text               I have never been more disrespected in my life...   \n",
       "original_length                                                          609   \n",
       "cleaned_length                                                           590   \n",
       "normalized_text            I have never been more disrespected in my life...   \n",
       "case_folded_text           i have never been more disrespected in my life...   \n",
       "tokens                     ['i', 'have', 'never', 'been', 'more', 'disres...   \n",
       "tokens_ml                  ['never', 'disrespected', 'life', 'first', 'co...   \n",
       "tokens_dl                  ['never', 'disrespected', 'life', 'first', 'co...   \n",
       "tokens_bert                ['i', 'have', 'never', 'been', 'more', 'disres...   \n",
       "lemmatized_ml              ['never', 'disrespect', 'life', 'first', 'cous...   \n",
       "lemmatized_dl              ['never', 'disrespect', 'life', 'first', 'cous...   \n",
       "lemmatized_bert            ['i', 'have', 'never', 'be', 'more', 'disrespe...   \n",
       "analysis_text              ['never', 'disrespect', 'life', 'first', 'cous...   \n",
       "vader_compound                                                        0.5774   \n",
       "vader_pos                                                              0.184   \n",
       "vader_neg                                                              0.094   \n",
       "vader_neu                                                              0.722   \n",
       "textblob_polarity                                                      0.225   \n",
       "textblob_subjectivity                                               0.316667   \n",
       "lexicon_score                                                              0   \n",
       "star_sentiment                                                          -1.0   \n",
       "combined_sentiment                                                  -0.08178   \n",
       "sentiment_label                                                     negative   \n",
       "sentiment_confidence                                                    0.25   \n",
       "date_only                                                         2016-10-26   \n",
       "hour                                                     2016-10-26 02:00:00   \n",
       "date_hour                                                2016-10-26 02:00:00   \n",
       "daily_count                                                                3   \n",
       "hourly_count                                                               1   \n",
       "daily_spike_score                                                   0.546965   \n",
       "hourly_spike_score                                                       0.0   \n",
       "temporal_spike_score                                                0.328179   \n",
       "user_review_count                                                          3   \n",
       "first_review                                             2016-10-23 21:01:00   \n",
       "last_review                                              2016-12-04 01:17:31   \n",
       "avg_user_rating                                                          1.0   \n",
       "positive_ratio                                                           0.0   \n",
       "activity_span_days                                                        42   \n",
       "review_frequency                                                    0.071429   \n",
       "suspicious_user                                                            0   \n",
       "user_behavior_score                                                 0.421237   \n",
       "text_length                                                               53   \n",
       "text_similarity_score                                                    0.4   \n",
       "rating_deviation                                                        0.75   \n",
       "user_rating_consistency                                                  0.0   \n",
       "extreme_rater_score                                                        1   \n",
       "rating_manipulation_score                                                0.5   \n",
       "consistency_score                                                        0.0   \n",
       "fake_probability_raw                                                0.336418   \n",
       "fake_probability                                                      0.8272   \n",
       "is_fake                                                                    1   \n",
       "fake_explanation               High rating deviation; Extreme rating pattern   \n",
       "\n",
       "                                                                        3937  \n",
       "business_name                                              Acme Oyster House  \n",
       "review_id                                             nY1p0rdzItxhzKJmyPknrQ  \n",
       "user_id                                               CaFbx2zBXiAUC-JmgsyDnQ  \n",
       "date                                                     2016-12-04 01:17:31  \n",
       "text                       I've never been more disrespected in my life, ...  \n",
       "stars_review                                                             1.0  \n",
       "stars_business_average                                                   4.0  \n",
       "cleaned_text               I have never been more disrespected in my life...  \n",
       "original_length                                                          609  \n",
       "cleaned_length                                                           590  \n",
       "normalized_text            I have never been more disrespected in my life...  \n",
       "case_folded_text           i have never been more disrespected in my life...  \n",
       "tokens                     ['i', 'have', 'never', 'been', 'more', 'disres...  \n",
       "tokens_ml                  ['never', 'disrespected', 'life', 'first', 'co...  \n",
       "tokens_dl                  ['never', 'disrespected', 'life', 'first', 'co...  \n",
       "tokens_bert                ['i', 'have', 'never', 'been', 'more', 'disres...  \n",
       "lemmatized_ml              ['never', 'disrespect', 'life', 'first', 'cous...  \n",
       "lemmatized_dl              ['never', 'disrespect', 'life', 'first', 'cous...  \n",
       "lemmatized_bert            ['i', 'have', 'never', 'be', 'more', 'disrespe...  \n",
       "analysis_text              ['never', 'disrespect', 'life', 'first', 'cous...  \n",
       "vader_compound                                                        0.5774  \n",
       "vader_pos                                                              0.184  \n",
       "vader_neg                                                              0.094  \n",
       "vader_neu                                                              0.722  \n",
       "textblob_polarity                                                      0.225  \n",
       "textblob_subjectivity                                               0.316667  \n",
       "lexicon_score                                                              0  \n",
       "star_sentiment                                                          -1.0  \n",
       "combined_sentiment                                                  -0.08178  \n",
       "sentiment_label                                                     negative  \n",
       "sentiment_confidence                                                    0.25  \n",
       "date_only                                                         2016-12-04  \n",
       "hour                                                     2016-12-04 01:00:00  \n",
       "date_hour                                                2016-12-04 01:00:00  \n",
       "daily_count                                                                5  \n",
       "hourly_count                                                               1  \n",
       "daily_spike_score                                                   1.955602  \n",
       "hourly_spike_score                                                       0.0  \n",
       "temporal_spike_score                                                1.173361  \n",
       "user_review_count                                                          3  \n",
       "first_review                                             2016-10-23 21:01:00  \n",
       "last_review                                              2016-12-04 01:17:31  \n",
       "avg_user_rating                                                          1.0  \n",
       "positive_ratio                                                           0.0  \n",
       "activity_span_days                                                        42  \n",
       "review_frequency                                                    0.071429  \n",
       "suspicious_user                                                            0  \n",
       "user_behavior_score                                                 0.421237  \n",
       "text_length                                                               53  \n",
       "text_similarity_score                                                    0.4  \n",
       "rating_deviation                                                        0.75  \n",
       "user_rating_consistency                                                  0.0  \n",
       "extreme_rater_score                                                        1  \n",
       "rating_manipulation_score                                                0.5  \n",
       "consistency_score                                                        0.0  \n",
       "fake_probability_raw                                                0.370178  \n",
       "fake_probability                                                    0.924719  \n",
       "is_fake                                                                    1  \n",
       "fake_explanation           High daily spike (z=1.96); High rating deviati...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loaded[df_loaded['text'].str.contains(\"I've never been more disrespected in my life\", case=False, na=False)].T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
